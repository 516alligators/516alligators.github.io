<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Ammar Plumber, Elaina Lin, Kim Nguyen, Meghan Aines, Ryan Karbowicz" />


<title>Homework 9 - Alligators</title>

<script src="site_libs/header-attrs-2.7/header-attrs.js"></script>
<link href="site_libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="site_libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="site_libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="site_libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









</head>

<body>




<h1 class="title toc-ignore">Homework 9 - Alligators</h1>
<h4 class="author">Ammar Plumber, Elaina Lin, Kim Nguyen, Meghan Aines, Ryan Karbowicz</h4>
<h4 class="date">4/13/2021</h4>



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>We will be examining the difference in tweet communications between TikTok and Facebook. These are two popular social media platforms but with very different target audiences. Thus, the two brands may differ in their communication styles and language. We set out to identify the particular ways in which they differ and to build a model that can attribute each tweet to the correct user.</p>
</div>
<div id="preliminary-analysis" class="section level1">
<h1>Preliminary Analysis</h1>
<p>First, we import all non-base packages to be used in this analysis.</p>
<pre class="r"><code>library(rtweet)
library(tidyverse)
library(lubridate)
library(scales)
library(tidytext)
library(wordcloud)
library(textdata)

library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(rpart.plot)
library(ipred)       # for fitting bagged decision trees
library(ranger)
library(gbm)
library(vip)

library(kableExtra)</code></pre>
<p>Now, we import the tweets that we pulled using the get_timeline() function and saved to a CSV file. There are ~3200 tweets from each user in our dataset.</p>
<pre class="r"><code># Run these two lines to get the tweets 
# and then save them as a csv for future use
# tiktok &lt;- get_timeline(&quot;tiktok_us&quot;, n=3200)
# tiktok %&gt;% write_as_csv(&#39;tiktok.csv&#39;)
# 
# facebook &lt;- get_timeline(&quot;Facebook&quot;, n=3200)
# facebook %&gt;% write_as_csv(&quot;facebook.csv&quot;)

tiktok &lt;-
  read_csv(&#39;tiktok.csv&#39;) %&gt;% 
  select(status_id, source, text, created_at) %&gt;% 
  as.data.frame()

facebook &lt;-
  read_csv(&#39;facebook.csv&#39;) %&gt;% 
  select(status_id, source, text, created_at)

nrc &lt;- read_rds(&quot;nrc.rds&quot;)

facebook %&gt;% head()</code></pre>
<pre><code>## # A tibble: 6 x 4
##   status_id        source      text                                                                     created_at         
##   &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;                                                                    &lt;dttm&gt;             
## 1 x13820200803434~ Twitter We~ &quot;Ramadan Mubarak &lt;U+0001F319&gt;\n \nThis #MonthofGood, check out all the ~ 2021-04-13 17:17:18
## 2 x13817344290186~ Khoros CX   &quot;@MeenalK1 Hi Meenal. Do you have the reference numbers for your submit~ 2021-04-12 22:22:13
## 3 x13817333826320~ Khoros CX   &quot;@Afrojalipro Thanks for updating us, Afroj! We&#39;re so happy to hear tha~ 2021-04-12 22:18:04
## 4 x13817326683881~ Khoros CX   &quot;@CallandManning Hi Calland. If you do not have access to the phone num~ 2021-04-12 22:15:14
## 5 x13817113768763~ Khoros CX   &quot;@BHARTINANDAN4 Hello! Please visit this Help Center article if you are~ 2021-04-12 20:50:37
## 6 x13817105484761~ Khoros CX   &quot;@weathermatt22 Hi Matt. Please visit our Help Center to report an issu~ 2021-04-12 20:47:20</code></pre>
<p>Now, for each user, we produce a line chart showing the percent of all tweets from each source by hour.</p>
<pre class="r"><code>facebook %&gt;%
  count(source, hour = hour(with_tz(created_at, &quot;EST&quot;))) %&gt;%
  mutate(percent = n/sum(n)) %&gt;%
  ggplot(aes(x = hour, y = percent, color = source)) +
  labs(x = &quot;Hour of day (EST)&quot;, y = &quot;% of tweets&quot;, color = &quot;&quot;) + 
  scale_y_continuous(labels = percent_format()) +
  geom_line() +
  ggtitle(&#39;Facebook Source Breakdown by Hour&#39;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-4-1.png" width="672"  /></p>
<pre class="r"><code>tiktok %&gt;%
  count(source, hour = hour(with_tz(created_at, &quot;EST&quot;))) %&gt;%
  mutate(percent = n/sum(n)) %&gt;%
  ggplot(aes(x = hour, y = percent, color = source)) +
  labs(x = &quot;Hour of day (EST)&quot;, y = &quot;% of tweets&quot;, color = &quot;&quot;) + 
  scale_y_continuous(labels = percent_format()) +
  geom_line() +
  ggtitle(&#39;Tiktok Source Breakdown by Hour&#39;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-4-2.png" width="672"  /></p>
<p>We see that the vast majority of Facebook’s tweets are put out using Khoros Publishing between the hours of 10 AM and 8 PM. TikTok publishes most of its tweets through the Twitter Web App and Fan Experiences Platform—usually between 10 AM and 8 PM, like Facebook.</p>
<p>We want to see if both users’ tweets tend to differ in length, so we create a histogram for each user.</p>
<pre class="r"><code>fb_wordcounts &lt;- 
  facebook %&gt;%
  mutate(tweetLength = str_length(text)) %&gt;% 
  filter(tweetLength &lt; 500)

tiktok_wordcounts &lt;- 
  tiktok %&gt;%
  mutate(tweetLength = str_length(text)) %&gt;% 
  filter(tweetLength &lt; 500)

writeLines(c(paste0(&quot;Facebook Mean Tweet Length: &quot;, 
                  mean(fb_wordcounts$tweetLength)), 
           paste0(&quot;TikTok Mean Tweet Length: &quot;, 
                  mean(tiktok_wordcounts$tweetLength))))</code></pre>
<pre><code>## Facebook Mean Tweet Length: 163.289555972483
## TikTok Mean Tweet Length: 112.557921102066</code></pre>
<pre class="r"><code>hist(tiktok_wordcounts$tweetLength, main = &quot;TikTok - Histogram of Tweet Lengths&quot;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-5-1.png" width="672"  /></p>
<pre class="r"><code>hist(fb_wordcounts$tweetLength, main = &quot;Facebook - Histogram of Tweet Lengths&quot;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-5-2.png" width="672"  /></p>
<p>As we see, TikTok’s tweet lengths are right-skewed, with most tweets being around 100 words long. Facebook, on the other hand, seems to post longer tweets, with a more normal distribution centered around 150 words long. Tweet length seems like a useful feature to include in our predictive model.</p>
<p>Next, we look at whether there is a difference in the share of Tweets that include pictures.</p>
<pre class="r"><code>fb_picture_counts &lt;- 
  facebook %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  count(picture = ifelse(str_detect(text, &quot;t.co&quot;),
                         &quot;Picture/link&quot;, &quot;No picture/link&quot;))

tiktok_picture_counts &lt;- 
  tiktok %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  count(picture = ifelse(str_detect(text, &quot;t.co&quot;),
                         &quot;Picture/link&quot;, &quot;No picture/link&quot;))

fb_pct_pics &lt;- 
  fb_picture_counts %&gt;% filter(picture == &quot;Picture/link&quot;) %&gt;% 
  select(2) %&gt;% as.numeric() / sum(fb_picture_counts$n) * 100

tiktok_pct_pics &lt;-
  tiktok_picture_counts %&gt;% filter(picture == &quot;Picture/link&quot;) %&gt;% 
  select(2) %&gt;% as.numeric() / sum(tiktok_picture_counts$n) * 100

cat(paste0(&quot;Percent of Tweets with pictures/link\n&quot;,
           &quot;\nFacebook: &quot;, round(fb_pct_pics, 2), 
           &quot;\nTikTok: &quot;, round(tiktok_pct_pics, 2)))</code></pre>
<pre><code>## Percent of Tweets with pictures/link
## 
## Facebook: 85.98
## TikTok: 52.33</code></pre>
<p>~86% of Facebook’s tweets contain pictures/links, while only ~52% of TikTok’s tweets contain pictures/links. This could be another useful predictor to include in our model.</p>
</div>
<div id="sentiment-analysis" class="section level1">
<h1>Sentiment Analysis</h1>
<p>Now, we split the tweets into tokens so that we can perform sentiment analysis on them.</p>
<pre class="r"><code>reg &lt;- &quot;([^A-Za-z\\d#@&#39;]|&#39;(?![A-Za-z\\d#@]))&quot;

# Unnest the text strings into a data frame of words
fb_words &lt;- 
  facebook %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  mutate(text = str_replace_all(text, 
                                &quot;https://t.co/[A-Za-z\\d]+|&amp;amp;&quot;, 
                                &quot;&quot;)) %&gt;%
  unnest_tokens(word, text, 
                token = &quot;regex&quot;, 
                pattern = reg) %&gt;%
  filter(!word %in% stop_words$word,
         str_detect(word, &quot;[a-z]&quot;))

tiktok_words &lt;- 
  tiktok %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  mutate(text = str_replace_all(text, 
                                &quot;https://t.co/[A-Za-z\\d]+|&amp;amp;&quot;, 
                                &quot;&quot;)) %&gt;%
  unnest_tokens(word, text, 
                token = &quot;regex&quot;, 
                pattern = reg) %&gt;%
  filter(!word %in% stop_words$word,
         str_detect(word, &quot;[a-z]&quot;))

# Inspect the first six rows of tweet_words
head(fb_words)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   status_id            source          created_at          word        
##   &lt;chr&gt;                &lt;chr&gt;           &lt;dttm&gt;              &lt;chr&gt;       
## 1 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 ramadan     
## 2 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 mubarak     
## 3 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 0001f319    
## 4 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 #monthofgood
## 5 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 check       
## 6 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 kindness</code></pre>
<p>We produce two horizontal bar graphs that show the most common words along with a wordcloud for each user.</p>
<pre class="r"><code>fb_words %&gt;%
  count(word, sort = TRUE) %&gt;%
  head(20) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(x = word, y = n)) +
  geom_bar(stat = &quot;identity&quot;) +
  ylab(&quot;Occurrences&quot;) +
  coord_flip() + 
  ggtitle(&quot;Facebook Word Frequency&quot;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-8-1.png" width="672"  /></p>
<pre class="r"><code>tiktok_words %&gt;%
  count(word, sort = TRUE) %&gt;%
  head(20) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(x = word, y = n)) +
  geom_bar(stat = &quot;identity&quot;) +
  ylab(&quot;Occurrences&quot;) +
  coord_flip() +
  ggtitle(&quot;TikTok Word Frequency&quot;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-8-2.png" width="672"  /></p>
<pre class="r"><code>facebook_cloud &lt;- 
  fb_words  %&gt;% 
  count(word) %&gt;% 
  arrange(-n)

wordcloud(facebook_cloud$word, 
          facebook_cloud$n, max.words = 200, 
          colors = c(&quot;#00B2FF&quot;, &quot;red&quot;, 
                     &quot;#FF0099&quot;, &quot;#6600CC&quot;, 
                     &quot;green&quot;, &quot;orange&quot;, 
                     &quot;blue&quot;, &quot;brown&quot;))</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-8-3.png" width="672"  /></p>
<pre class="r"><code>tiktok_cloud &lt;- 
  tiktok_words  %&gt;% 
  count(word) %&gt;% 
  arrange(-n)

wordcloud(tiktok_cloud$word, 
          tiktok_cloud$n, max.words = 200, 
          colors = c(&quot;#00B2FF&quot;, &quot;red&quot;, 
                     &quot;#FF0099&quot;, &quot;#6600CC&quot;, 
                     &quot;green&quot;, &quot;orange&quot;, 
                     &quot;blue&quot;, &quot;brown&quot;))</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-8-4.png" width="672"  /></p>
<p>We join the NRC Word-Emotion Association Lexicon to our data, which will allow us to identify words associated with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).</p>
<pre class="r"><code>fb_sentiment &lt;-
  inner_join(fb_words, nrc, by = &quot;word&quot;)

tiktok_sentiment &lt;-
  inner_join(tiktok_words, nrc, by = &quot;word&quot;)

fb_sentiment %&gt;% head()</code></pre>
<pre><code>## # A tibble: 6 x 5
##   status_id            source          created_at          word      sentiment   
##   &lt;chr&gt;                &lt;chr&gt;           &lt;dttm&gt;              &lt;chr&gt;     &lt;chr&gt;       
## 1 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 kindness  positive    
## 2 x1381733382632001536 Khoros CX       2021-04-12 22:18:04 happy     anticipation
## 3 x1381733382632001536 Khoros CX       2021-04-12 22:18:04 happy     joy         
## 4 x1381733382632001536 Khoros CX       2021-04-12 22:18:04 happy     positive    
## 5 x1381733382632001536 Khoros CX       2021-04-12 22:18:04 happy     trust       
## 6 x1381733382632001536 Khoros CX       2021-04-12 22:18:04 wonderful joy</code></pre>
<p>Here, we compare Facebook’s and TikTok’s sentiments.</p>
<pre class="r"><code>fb_sentiment_analysis &lt;- 
  fb_sentiment %&gt;% 
  count(word, sentiment) %&gt;% 
  group_by(sentiment)

fb_sentiment_analysis %&gt;%  
  top_n(15) %&gt;% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = &quot;identity&quot;) +
  coord_flip() +
  ylab(&quot;Frequency&quot;) +
  xlab(&quot;Sentiment&quot;) +
  labs(title=&quot;Facebook Sentiment&quot;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-10-1.png" width="672"  /></p>
<pre class="r"><code>tiktok_sentiment_analysis &lt;- 
  tiktok_sentiment %&gt;% 
  count(word, sentiment) %&gt;% 
  group_by(sentiment)

tiktok_sentiment_analysis %&gt;%  
  top_n(15) %&gt;% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = &quot;identity&quot;) +
  coord_flip() +
  ylab(&quot;Frequency&quot;) +
  xlab(&quot;Sentiment&quot;) +
  labs(title=&quot;TikTok Sentiment&quot;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-10-2.png" width="672"  /></p>
<p>It looks like Facebook’s tweets use more trust words while TikTok uses more words that reflect anticipation. We now show specifically which words are conveying each of these observed sentiments.</p>
<pre class="r"><code>fb_sentiment_analysis %&gt;% filter(!sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %&gt;% top_n(10) -&gt; fb_sentiment_analysis2

ggplot(fb_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = &quot;free&quot;)+ 
  geom_bar(stat =&quot;identity&quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y=&quot;count&quot;, title=&quot;Facebook Sentiment Words&quot;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-11-1.png" width="672"  /></p>
<pre class="r"><code>tiktok_sentiment_analysis %&gt;% filter(!sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %&gt;% top_n(10) -&gt; tiktok_sentiment_analysis2

ggplot(tiktok_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = &quot;free&quot;)+ 
  geom_bar(stat =&quot;identity&quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y=&quot;count&quot;, title=&quot;Tik Tok Sentiment Words&quot;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-11-2.png" width="672"  /></p>
<p>Next, we examine texts on Facebook and Tiktok to see their positive-negative score by using the AFINN sentiment lexicon, a list of English terms manually rated for valence with an integer between -5 (negative) and +5 (positive) by Finn Årup Nielsen between 2009 and 2011.</p>
<p>We use this lexicon to compute mean positivity scores for all words tweeted by each user.</p>
<pre class="r"><code># run this to get afinn lexicon and save it as a csv
# get_sentiments (&quot;afinn&quot;) -&gt; afinn
#
#afinn %&gt;% write_as_csv(&quot;afinn.csv&quot;)

afinn &lt;- read_csv(&#39;afinn.csv&#39;)

fb_afinn &lt;-    
 inner_join(fb_words, 
            afinn, 
            by = &quot;word&quot;)

tiktok_afinn &lt;-    
 inner_join(tiktok_words, 
            afinn, 
            by = &quot;word&quot;)

fb_mean_afinn &lt;- 
  fb_afinn %&gt;% 
  summarise(mean_fb_afinn = mean(value))

tiktok_mean_afinn &lt;- 
  tiktok_afinn %&gt;% 
  summarise(mean_tt_afinn = mean(value))

cat(paste0(&quot;Average AFINN scores for all words by user\n&quot;,
           &quot;\nFacebook: &quot;, round(fb_mean_afinn, 3), 
           &quot;\nTikTok: &quot;, round(tiktok_mean_afinn, 3)))</code></pre>
<pre><code>## Average AFINN scores for all words by user
## 
## Facebook: 0.785
## TikTok: 1.704</code></pre>
<p>Facebook’s mean AFINN value is 0.79 while TikTok’s mean AFINN value is 1.704. In general, words tweeted by Tiktok are more positive than those tweeted by Facebook.</p>
</div>
<div id="training-predictive-models" class="section level1">
<h1>Training Predictive Models</h1>
<p>Here, using the text of a tweet, we attempt to predict the user who tweeted it.</p>
<p>The features we extracted are tweet length, the presence of a picture/link, number of words for each sentiment, and mean AFINN score per tweet.</p>
<p>TikTok is encoded as 1, and Facebook is encoded as 0.</p>
<p>First, we produce a simple decision tree.</p>
<pre class="r"><code>fb_piclinks &lt;-
  facebook %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  mutate(picture_link = ifelse(str_detect(text, &quot;t.co&quot;),
                         1, 0)) %&gt;% 
  select(1,5)

tiktok_piclinks &lt;- 
  tiktok %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  mutate(picture_link = ifelse(str_detect(text, &quot;t.co&quot;),
                         1, 0)) %&gt;% 
  select(1,5)

fb_tweet_afinn &lt;- 
  fb_afinn %&gt;% 
  group_by(status_id) %&gt;% 
  summarize(afinn = mean(value))

tiktok_tweet_afinn &lt;- 
  tiktok_afinn %&gt;% 
  group_by(status_id) %&gt;% 
  summarize(afinn = mean(value))

fb_sentiment_counts &lt;- 
  fb_sentiment %&gt;% 
  group_by(status_id) %&gt;% 
  count(sentiment) %&gt;% 
  ungroup() %&gt;% 
  pivot_wider(id_cols = status_id, 
              names_from = sentiment, 
              values_from = n,
              values_fill = 0)

tiktok_sentiment_counts &lt;- 
  tiktok_sentiment %&gt;% 
  group_by(status_id) %&gt;% 
  count(sentiment) %&gt;% 
  ungroup() %&gt;% 
  pivot_wider(id_cols = status_id, 
              names_from = sentiment, 
              values_from = n,
              values_fill = 0)

tiktok_feature_selection &lt;- 
  tiktok_wordcounts %&gt;% 
  mutate(user = 1) %&gt;% 
  left_join(tiktok_sentiment_counts, 
            by=&quot;status_id&quot;) %&gt;% 
  left_join(tiktok_tweet_afinn,
            by=&quot;status_id&quot;) %&gt;% 
  left_join(tiktok_piclinks,
            by=&quot;status_id&quot;)

facebook_feature_selection &lt;-
  fb_wordcounts %&gt;% 
  mutate(user = 0) %&gt;% 
  left_join(fb_sentiment_counts, 
            by=&quot;status_id&quot;) %&gt;% 
  left_join(fb_tweet_afinn,
            by=&quot;status_id&quot;) %&gt;% 
  left_join(fb_piclinks,
            by=&quot;status_id&quot;)

both_users &lt;- 
  tiktok_feature_selection %&gt;% 
  rbind(facebook_feature_selection) %&gt;%
  mutate_if(is.numeric,coalesce,0)

set.seed(123)
index &lt;- 
  createDataPartition(both_users$user,
                      p = 0.8, list = FALSE)

for_decisiontree &lt;-
  both_users %&gt;% select(-1,-2,-3,-4) %&gt;% 
  drop_na()

train &lt;- for_decisiontree[index, ]
test  &lt;- for_decisiontree[-index, ]

set.seed(123)
simple_model &lt;- rpart(user ~ ., 
                      data = train, method = &quot;class&quot;)
rpart.plot(simple_model, yesno = 2)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-13-1.png" width="672"  /></p>
<p>We produce additional models using the bagging, random forests, and gradient boosting methods.</p>
<pre class="r"><code>set.seed(123)
bagging_model &lt;- train(
  user ~ .,
  data = train,
  method = &quot;treebag&quot;,
  trControl = trainControl(method = &quot;oob&quot;),
  keepX = T,
  nbagg = 100,
  importance = &quot;impurity&quot;,
  control = rpart.control(minsplit = 2, cp = 0)
)

n_features &lt;- length(setdiff(names(train), &quot;user&quot;))

train$user &lt;- as.factor(train$user)
rf_model &lt;- ranger(
  user ~ .,
  data = train,
  mtry = floor(n_features * 0.5),
  respect.unordered.factors = &quot;order&quot;,
  importance = &quot;permutation&quot;,
  seed = 123
)


set.seed(123)  # for reproducibility
train$user &lt;- as.numeric(train$user)-1
gbm_model &lt;- gbm(
  formula = user ~ .,
  data = train,
  distribution = &quot;gaussian&quot;,  # SSE loss function
  n.trees = 1000,
  shrinkage = 0.05,
  interaction.depth = 5,
  n.minobsinnode = 4,
  cv.folds = 10
)</code></pre>
<p>We also display four variable importance plots to see which variables each model identified as significant.</p>
<pre class="r"><code>vip(simple_model, num_features = 30) + 
  ggtitle(&#39;Simple Decision Tree - Variable Importance Plot&#39;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-15-1.png" width="672"  /></p>
<pre class="r"><code>vip(bagging_model, num_features = 30) + 
  ggtitle(&#39;Bagging - Variable Importance Plot&#39;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-15-2.png" width="672"  /></p>
<pre class="r"><code>vip(rf_model, num_features = 30) + 
  ggtitle(&#39;Random Forests - Variable Importance Plot&#39;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-15-3.png" width="672"  /></p>
<pre class="r"><code>vip(gbm_model, num_features = 30) + 
  ggtitle(&#39;Gradient Boosting - Variable Importance Plot&#39;)</code></pre>
<p><img src="HW9_files/figure-html/unnamed-chunk-15-4.png" width="672"  /></p>
<p>It seems that the simple decision tree placed the most importance on the presence of a picture/link. The bagging model, on the other hand places no importance on this variable. All four methods identified tweet length as strongly predictive of the user. All four heavily weighted anticipation sentiments and AFINN scores.</p>
</div>
<div id="results-and-discussion" class="section level1">
<h1>Results and Discussion</h1>
<p>Now, I produce confusion matrices and show residual sum of squares for all tree-based methods—first evaluating their performance on the training set and then on the test set. Note again that a Tiktok tweet is encoded as 1, and a Facebook tweet is encoded as 0. The code is shown for the first matrix but not for subsequent ones for the sake of elegance.</p>
<div id="training-set-performance" class="section level2">
<h2>Training Set Performance</h2>
<p><strong>Simple Decision Tree - Training Set:</strong></p>
<pre><code>## [1] 1 0
## Levels: 0 1</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 2209  574
##          1  338 1993
##                                           
##                Accuracy : 0.8217          
##                  95% CI : (0.8109, 0.8321)
##     No Information Rate : 0.502           
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.6435          
##                                           
##  Mcnemar&#39;s Test P-Value : 7.16e-15        
##                                           
##               Precision : 0.7937          
##                  Recall : 0.8673          
##                      F1 : 0.8289          
##              Prevalence : 0.4980          
##          Detection Rate : 0.4320          
##    Detection Prevalence : 0.5442          
##       Balanced Accuracy : 0.8218          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Bagging Method - Training Set:</strong></p>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 2462   59
##          1   85 2508
##                                           
##                Accuracy : 0.9718          
##                  95% CI : (0.9669, 0.9762)
##     No Information Rate : 0.502           
##     P-Value [Acc &gt; NIR] : &lt; 2e-16         
##                                           
##                   Kappa : 0.9437          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.03722         
##                                           
##               Precision : 0.9766          
##                  Recall : 0.9666          
##                      F1 : 0.9716          
##              Prevalence : 0.4980          
##          Detection Rate : 0.4814          
##    Detection Prevalence : 0.4930          
##       Balanced Accuracy : 0.9718          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Random Forests Method - Training Set:</strong></p>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 2429  106
##          1  118 2461
##                                           
##                Accuracy : 0.9562          
##                  95% CI : (0.9502, 0.9616)
##     No Information Rate : 0.502           
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9124          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.4624          
##                                           
##               Precision : 0.9582          
##                  Recall : 0.9537          
##                      F1 : 0.9559          
##              Prevalence : 0.4980          
##          Detection Rate : 0.4750          
##    Detection Prevalence : 0.4957          
##       Balanced Accuracy : 0.9562          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Gradient Boosting Method - Training Set:</strong></p>
<pre><code>## Using 979 trees...</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 2311  282
##          1  236 2285
##                                           
##                Accuracy : 0.8987          
##                  95% CI : (0.8901, 0.9068)
##     No Information Rate : 0.502           
##     P-Value [Acc &gt; NIR] : &lt; 2e-16         
##                                           
##                   Kappa : 0.7974          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.04802         
##                                           
##               Precision : 0.8912          
##                  Recall : 0.9073          
##                      F1 : 0.8992          
##              Prevalence : 0.4980          
##          Detection Rate : 0.4519          
##    Detection Prevalence : 0.5070          
##       Balanced Accuracy : 0.8987          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Performance Summary and RSS</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
type
</th>
<th style="text-align:right;">
total_errors
</th>
<th style="text-align:right;">
accuracy
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Simple
</td>
<td style="text-align:right;">
912
</td>
<td style="text-align:right;">
0.8216660
</td>
</tr>
<tr>
<td style="text-align:left;">
Bagging
</td>
<td style="text-align:right;">
144
</td>
<td style="text-align:right;">
0.9718420
</td>
</tr>
<tr>
<td style="text-align:left;">
Random Forests
</td>
<td style="text-align:right;">
224
</td>
<td style="text-align:right;">
0.9561987
</td>
</tr>
<tr>
<td style="text-align:left;">
Gradient Boosting
</td>
<td style="text-align:right;">
518
</td>
<td style="text-align:right;">
0.8987094
</td>
</tr>
</tbody>
</table>
<p>The rankings for accuracy on the training set are as follows: 1. Bagging method 2. Random forests 3. Gradient boosting method 4. Simple decision tree</p>
<p>We show the residual sum of squares for all four models on the training set below.</p>
<pre class="r"><code>rss_simple_train &lt;- sum((actual_train-simple_pred_train)^2)
rss_bagging_train &lt;- sum((actual_train-bagging_pred_train)^2)
rss_rf_train &lt;- sum((actual_train-rf_pred_train)^2)
rss_gb_train &lt;- sum((actual_train-gb_pred_train)^2)

cat(paste0(&quot;Residual Sum of Squares on Training Set\n&quot;,
           &quot;\nSimple model: &quot;, rss_simple_train, 
           &quot;\nBagging model: &quot;, rss_bagging_train, 
           &quot;\nRandom forests model: &quot;, rss_rf_train, 
           &quot;\nGradient boosting model: &quot;, rss_gb_train))</code></pre>
<pre><code>## Residual Sum of Squares on Training Set
## 
## Simple model: 743.785102129366
## Bagging model: 147.807476444236
## Random forests model: 224
## Gradient boosting model: 390.0864919704</code></pre>
<p>The bagging model performed the best on the training set, followed by the random forests method, the gradient boosting method, and the simple model in last place.</p>
<p>Now, we show confusion matrices for the test set.</p>
</div>
<div id="test-set-performance" class="section level2">
<h2>Test Set Performance</h2>
<p><strong>Simple Decision Tree - Test Set:</strong></p>
<pre class="r"><code>actual_test &lt;- test$user

simple_pred_test &lt;- 
  predict(simple_model, newdata = test) %&gt;% 
  as_tibble() %&gt;% 
  select(2) %&gt;% 
  unlist() %&gt;% 
  as.vector()

simple_test_confusion &lt;- 
  confusionMatrix(data = factor(round(simple_pred_test)),
                  reference = factor(actual_test), mode = &quot;prec_recall&quot;)

simple_test_errors &lt;- 
  simple_test_confusion$table[2] +
  simple_test_confusion$table[3]

simple_test_accuracy &lt;-
  as.numeric(simple_test_confusion$overall[1])

simple_test_confusion</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 563 135
##          1  88 492
##                                           
##                Accuracy : 0.8255          
##                  95% CI : (0.8036, 0.8459)
##     No Information Rate : 0.5094          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.6504          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.002067        
##                                           
##               Precision : 0.8066          
##                  Recall : 0.8648          
##                      F1 : 0.8347          
##              Prevalence : 0.5094          
##          Detection Rate : 0.4405          
##    Detection Prevalence : 0.5462          
##       Balanced Accuracy : 0.8248          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Bagging Method - Test Set:</strong></p>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 550 102
##          1 101 525
##                                           
##                Accuracy : 0.8412          
##                  95% CI : (0.8199, 0.8608)
##     No Information Rate : 0.5094          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.6822          
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##               Precision : 0.8436          
##                  Recall : 0.8449          
##                      F1 : 0.8442          
##              Prevalence : 0.5094          
##          Detection Rate : 0.4304          
##    Detection Prevalence : 0.5102          
##       Balanced Accuracy : 0.8411          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Random Forests Method - Test Set:</strong></p>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 572  92
##          1  79 535
##                                           
##                Accuracy : 0.8662          
##                  95% CI : (0.8463, 0.8844)
##     No Information Rate : 0.5094          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.7322          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.3588          
##                                           
##               Precision : 0.8614          
##                  Recall : 0.8786          
##                      F1 : 0.8700          
##              Prevalence : 0.5094          
##          Detection Rate : 0.4476          
##    Detection Prevalence : 0.5196          
##       Balanced Accuracy : 0.8660          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Gradient Boosting Method - Test Set:</strong></p>
<pre><code>## Using 979 trees...</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 569  84
##          1  82 543
##                                           
##                Accuracy : 0.8701          
##                  95% CI : (0.8504, 0.8881)
##     No Information Rate : 0.5094          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.7401          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.9381          
##                                           
##               Precision : 0.8714          
##                  Recall : 0.8740          
##                      F1 : 0.8727          
##              Prevalence : 0.5094          
##          Detection Rate : 0.4452          
##    Detection Prevalence : 0.5110          
##       Balanced Accuracy : 0.8700          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Performance Summary and RSS</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
type
</th>
<th style="text-align:right;">
total_errors
</th>
<th style="text-align:right;">
accuracy
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Simple
</td>
<td style="text-align:right;">
223
</td>
<td style="text-align:right;">
0.8255086
</td>
</tr>
<tr>
<td style="text-align:left;">
Bagging
</td>
<td style="text-align:right;">
203
</td>
<td style="text-align:right;">
0.8411581
</td>
</tr>
<tr>
<td style="text-align:left;">
Random Forests
</td>
<td style="text-align:right;">
171
</td>
<td style="text-align:right;">
0.8661972
</td>
</tr>
<tr>
<td style="text-align:left;">
Gradient Boosting
</td>
<td style="text-align:right;">
166
</td>
<td style="text-align:right;">
0.8701095
</td>
</tr>
</tbody>
</table>
<p>The rankings for accuracy on the test set are as follows:</p>
<ol style="list-style-type: decimal">
<li>Random forests</li>
<li>Gradient boosting method</li>
<li>Bagging method</li>
<li>Simple decision tree</li>
</ol>
<p>Now, we show the residual sum of squares for each model with respect to the test set.</p>
<pre class="r"><code>rss_simple_test &lt;- sum((actual_test-simple_pred_test)^2)
rss_bagging_test &lt;- sum((actual_test-bagging_pred_test)^2)
rss_rf_test &lt;- sum((actual_test-rf_pred_test)^2)
rss_gb_test &lt;- sum((actual_test-gb_pred_test)^2)

cat(paste0(&quot;Residual Sum of Squares on Test Set\n&quot;,
           &quot;\nSimple model: &quot;, rss_simple_test, 
           &quot;\nBagged model: &quot;, rss_bagging_test, 
           &quot;\nRandom forests model: &quot;, rss_rf_test, 
           &quot;\nGradient boost model: &quot;, rss_gb_test))</code></pre>
<pre><code>## Residual Sum of Squares on Test Set
## 
## Simple model: 183.394951302835
## Bagged model: 139.545475352265
## Random forests model: 171
## Gradient boost model: 131.566142920475</code></pre>
<p>The random forests model performed the best on the test set even though it was only second best for the training set. However, that may be an indication that the bagging model was overfit to the training data, which caused it to perform worse on the test set than the random forests model.</p>
<p>In sum, it seems that the best model is the random forests model, with a test set accuracy score of 86.62%.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Looking at the analyses, it seems that the Facebook and TikTok accounts have systematically different Twitter presences. Facebook seems to respond more frequently to user fears, which are associated with words such as “secure” and “trust.” Whereas, TikTok focuses on generating excitement and offer prize giveaways, which is associated with “anticipation” words such as “winning” and “tomorrow.” Differences in tweet length also possibly reflect on the preferences of the target audience; TikTok users are younger and less likely to consume written information (it is a video platform, after all), and the opposite is true for Facebook. In sum, our predictive endeavor was successful, and we unveiled a number of useful insights from it.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
